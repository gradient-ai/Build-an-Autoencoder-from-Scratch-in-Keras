{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Build a Variational Autoencoder in Keras\n",
    "For a detailed tutorial, check out the post [on the blog](https://blog.paperspace.com/how-to-build-variational-autoencoder-keras/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aCshAL5ZlnvE"
   },
   "source": [
    "## Building the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "colab_type": "code",
    "id": "M2RVB2Vvj3y3",
    "outputId": "2a0b1722-8825-4f6c-84b3-d78e758d4e5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 28, 28, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_1 (Conv2D)         (None, 28, 28, 1)    10          encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_norm_1 (BatchNormalizat (None, 28, 28, 1)    4           encoder_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_leakyrelu_1 (LeakyReLU) (None, 28, 28, 1)    0           encoder_norm_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_2 (Conv2D)         (None, 28, 28, 32)   320         encoder_leakyrelu_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "encoder_norm_2 (BatchNormalizat (None, 28, 28, 32)   128         encoder_conv_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_activ_layer_2 (LeakyReL (None, 28, 28, 32)   0           encoder_norm_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_3 (Conv2D)         (None, 14, 14, 64)   18496       encoder_activ_layer_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_norm_3 (BatchNormalizat (None, 14, 14, 64)   256         encoder_conv_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_activ_layer_3 (LeakyReL (None, 14, 14, 64)   0           encoder_norm_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_4 (Conv2D)         (None, 7, 7, 64)     36928       encoder_activ_layer_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_norm_4 (BatchNormalizat (None, 7, 7, 64)     256         encoder_conv_4[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_activ_layer_4 (LeakyReL (None, 7, 7, 64)     0           encoder_norm_4[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_5 (Conv2D)         (None, 7, 7, 64)     36928       encoder_activ_layer_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_norm_5 (BatchNormalizat (None, 7, 7, 64)     256         encoder_conv_5[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_activ_layer_5 (LeakyReL (None, 7, 7, 64)     0           encoder_norm_5[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 3136)         0           encoder_activ_layer_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_mu (Dense)              (None, 2)            6274        flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "encoder_log_variance (Dense)    (None, 2)            6274        flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "encoder_output (Lambda)         (None, 2)            0           encoder_mu[0][0]                 \n",
      "                                                                 encoder_log_variance[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 106,130\n",
      "Trainable params: 105,680\n",
      "Non-trainable params: 450\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import numpy\n",
    "import matplotlib.pyplot\n",
    "\n",
    "img_size = 28\n",
    "num_channels = 1\n",
    "latent_space_dim = 2\n",
    "\n",
    "# Encoder\n",
    "x = tensorflow.keras.layers.Input(shape=(img_size, img_size, num_channels), name=\"encoder_input\")\n",
    "\n",
    "encoder_conv_layer1 = tensorflow.keras.layers.Conv2D(filters=1, kernel_size=(3, 3), padding=\"same\", strides=1, name=\"encoder_conv_1\")(x)\n",
    "encoder_norm_layer1 = tensorflow.keras.layers.BatchNormalization(name=\"encoder_norm_1\")(encoder_conv_layer1)\n",
    "encoder_activ_layer1 = tensorflow.keras.layers.LeakyReLU(name=\"encoder_leakyrelu_1\")(encoder_norm_layer1)\n",
    "\n",
    "encoder_conv_layer2 = tensorflow.keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding=\"same\", strides=1, name=\"encoder_conv_2\")(encoder_activ_layer1)\n",
    "encoder_norm_layer2 = tensorflow.keras.layers.BatchNormalization(name=\"encoder_norm_2\")(encoder_conv_layer2)\n",
    "encoder_activ_layer2 = tensorflow.keras.layers.LeakyReLU(name=\"encoder_activ_layer_2\")(encoder_norm_layer2)\n",
    "\n",
    "encoder_conv_layer3 = tensorflow.keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", strides=2, name=\"encoder_conv_3\")(encoder_activ_layer2)\n",
    "encoder_norm_layer3 = tensorflow.keras.layers.BatchNormalization(name=\"encoder_norm_3\")(encoder_conv_layer3)\n",
    "encoder_activ_layer3 = tensorflow.keras.layers.LeakyReLU(name=\"encoder_activ_layer_3\")(encoder_norm_layer3)\n",
    "\n",
    "encoder_conv_layer4 = tensorflow.keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", strides=2, name=\"encoder_conv_4\")(encoder_activ_layer3)\n",
    "encoder_norm_layer4 = tensorflow.keras.layers.BatchNormalization(name=\"encoder_norm_4\")(encoder_conv_layer4)\n",
    "encoder_activ_layer4 = tensorflow.keras.layers.LeakyReLU(name=\"encoder_activ_layer_4\")(encoder_norm_layer4)\n",
    "\n",
    "encoder_conv_layer5 = tensorflow.keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", strides=1, name=\"encoder_conv_5\")(encoder_activ_layer4)\n",
    "encoder_norm_layer5 = tensorflow.keras.layers.BatchNormalization(name=\"encoder_norm_5\")(encoder_conv_layer5)\n",
    "encoder_activ_layer5 = tensorflow.keras.layers.LeakyReLU(name=\"encoder_activ_layer_5\")(encoder_norm_layer5)\n",
    "\n",
    "shape_before_flatten = tensorflow.keras.backend.int_shape(encoder_activ_layer5)[1:]\n",
    "encoder_flatten = tensorflow.keras.layers.Flatten()(encoder_activ_layer5)\n",
    "\n",
    "encoder_mu = tensorflow.keras.layers.Dense(units=latent_space_dim, name=\"encoder_mu\")(encoder_flatten)\n",
    "encoder_log_variance = tensorflow.keras.layers.Dense(units=latent_space_dim, name=\"encoder_log_variance\")(encoder_flatten)\n",
    "\n",
    "encoder_mu_log_variance_model = tensorflow.keras.models.Model(x, (encoder_mu, encoder_log_variance), name=\"encoder_mu_log_variance_model\")\n",
    "\n",
    "def sampling(mu_log_variance):\n",
    "    mu, log_variance = mu_log_variance\n",
    "    epsilon = tensorflow.keras.backend.random_normal(shape=tensorflow.keras.backend.shape(mu), mean=0.0, stddev=1.0)\n",
    "    random_sample = mu + tensorflow.keras.backend.exp(log_variance/2) * epsilon\n",
    "    return random_sample\n",
    "\n",
    "encoder_output = tensorflow.keras.layers.Lambda(sampling, name=\"encoder_output\")([encoder_mu, encoder_log_variance])\n",
    "\n",
    "encoder = tensorflow.keras.models.Model(x, encoder_output, name=\"encoder_model\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wY-qYSYblrFs"
   },
   "source": [
    "## Building the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "colab_type": "code",
    "id": "6J4TtB1kj5we",
    "outputId": "93add8f9-3c1d-4a89-f6a0-7f9d090191b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "decoder_dense_1 (Dense)      (None, 3136)              9408      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "decoder_conv_tran_1 (Conv2DT (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "decoder_norm_1 (BatchNormali (None, 7, 7, 64)          256       \n",
      "_________________________________________________________________\n",
      "decoder_leakyrelu_1 (LeakyRe (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "decoder_conv_tran_2 (Conv2DT (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "decoder_norm_2 (BatchNormali (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "decoder_leakyrelu_2 (LeakyRe (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "decoder_conv_tran_3 (Conv2DT (None, 28, 28, 64)        36928     \n",
      "_________________________________________________________________\n",
      "decoder_norm_3 (BatchNormali (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "decoder_leakyrelu_3 (LeakyRe (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "decoder_conv_tran_4 (Conv2DT (None, 28, 28, 1)         577       \n",
      "_________________________________________________________________\n",
      "decoder_output (LeakyReLU)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 121,537\n",
      "Trainable params: 121,153\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_input = tensorflow.keras.layers.Input(shape=(latent_space_dim), name=\"decoder_input\")\n",
    "decoder_dense_layer1 = tensorflow.keras.layers.Dense(units=numpy.prod(shape_before_flatten), name=\"decoder_dense_1\")(decoder_input)\n",
    "decoder_reshape = tensorflow.keras.layers.Reshape(target_shape=shape_before_flatten)(decoder_dense_layer1)\n",
    "\n",
    "decoder_conv_tran_layer1 = tensorflow.keras.layers.Conv2DTranspose(filters=64, kernel_size=(3, 3), padding=\"same\", strides=1, name=\"decoder_conv_tran_1\")(decoder_reshape)\n",
    "decoder_norm_layer1 = tensorflow.keras.layers.BatchNormalization(name=\"decoder_norm_1\")(decoder_conv_tran_layer1)\n",
    "decoder_activ_layer1 = tensorflow.keras.layers.LeakyReLU(name=\"decoder_leakyrelu_1\")(decoder_norm_layer1)\n",
    "\n",
    "decoder_conv_tran_layer2 = tensorflow.keras.layers.Conv2DTranspose(filters=64, kernel_size=(3, 3), padding=\"same\", strides=2, name=\"decoder_conv_tran_2\")(decoder_activ_layer1)\n",
    "decoder_norm_layer2 = tensorflow.keras.layers.BatchNormalization(name=\"decoder_norm_2\")(decoder_conv_tran_layer2)\n",
    "decoder_activ_layer2 = tensorflow.keras.layers.LeakyReLU(name=\"decoder_leakyrelu_2\")(decoder_norm_layer2)\n",
    "\n",
    "decoder_conv_tran_layer3 = tensorflow.keras.layers.Conv2DTranspose(filters=64, kernel_size=(3, 3), padding=\"same\", strides=2, name=\"decoder_conv_tran_3\")(decoder_activ_layer2)\n",
    "decoder_norm_layer3 = tensorflow.keras.layers.BatchNormalization(name=\"decoder_norm_3\")(decoder_conv_tran_layer3)\n",
    "decoder_activ_layer3 = tensorflow.keras.layers.LeakyReLU(name=\"decoder_leakyrelu_3\")(decoder_norm_layer3)\n",
    "\n",
    "decoder_conv_tran_layer4 = tensorflow.keras.layers.Conv2DTranspose(filters=1, kernel_size=(3, 3), padding=\"same\", strides=1, name=\"decoder_conv_tran_4\")(decoder_activ_layer3)\n",
    "decoder_output = tensorflow.keras.layers.LeakyReLU(name=\"decoder_output\")(decoder_conv_tran_layer4 )\n",
    "\n",
    "decoder = tensorflow.keras.models.Model(decoder_input, decoder_output, name=\"decoder_model\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Inw7qEUlkEc"
   },
   "source": [
    "## Building the VAE out of the Encoder and the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "ufkT79GPkgpH",
    "outputId": "67bc5c68-2269-475a-cb9c-2134e386585d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"VAE\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "VAE_input (InputLayer)       [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "encoder_model (Model)        (None, 2)                 106130    \n",
      "_________________________________________________________________\n",
      "decoder_model (Model)        (None, 28, 28, 1)         121537    \n",
      "=================================================================\n",
      "Total params: 227,667\n",
      "Trainable params: 226,833\n",
      "Non-trainable params: 834\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae_input = tensorflow.keras.layers.Input(shape=(img_size, img_size, num_channels), name=\"VAE_input\")\n",
    "vae_encoder_output = encoder(vae_input)\n",
    "vae_decoder_output = decoder(vae_encoder_output)\n",
    "vae = tensorflow.keras.models.Model(vae_input, vae_decoder_output, name=\"VAE\")\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yb3RzAwYlgjl"
   },
   "source": [
    "## Loss Function & Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ejtyOLYPkm6c"
   },
   "outputs": [],
   "source": [
    "def loss_func(encoder_mu, encoder_log_variance):\n",
    "    def vae_reconstruction_loss(y_true, y_predict):\n",
    "        reconstruction_loss_factor = 1000\n",
    "        reconstruction_loss = tensorflow.keras.backend.mean(tensorflow.keras.backend.square(y_true-y_predict), axis=[1, 2, 3])\n",
    "        return reconstruction_loss_factor * reconstruction_loss\n",
    "\n",
    "    def vae_kl_loss(encoder_mu, encoder_log_variance):\n",
    "        kl_loss = -0.5 * tensorflow.keras.backend.sum(1.0 + encoder_log_variance - tensorflow.keras.backend.square(encoder_mu) - tensorflow.keras.backend.exp(encoder_log_variance), axis=1)\n",
    "        return kl_loss\n",
    "\n",
    "    def vae_kl_loss_metric(y_true, y_predict):\n",
    "        kl_loss = -0.5 * tensorflow.keras.backend.sum(1.0 + encoder_log_variance - tensorflow.keras.backend.square(encoder_mu) - tensorflow.keras.backend.exp(encoder_log_variance), axis=1)\n",
    "        return kl_loss\n",
    "\n",
    "    def vae_loss(y_true, y_predict):\n",
    "        reconstruction_loss = vae_reconstruction_loss(y_true, y_predict)\n",
    "        kl_loss = vae_kl_loss(y_true, y_predict)\n",
    "\n",
    "        loss = reconstruction_loss + kl_loss\n",
    "        return loss\n",
    "\n",
    "    return vae_loss\n",
    "\n",
    "vae.compile(optimizer=tensorflow.keras.optimizers.Adam(lr=0.0005), loss=loss_func(encoder_mu, encoder_log_variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gURyli7_ldAv"
   },
   "source": [
    "## Loading the Data and Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "uuxiRs1Ukrvb",
    "outputId": "61b72af7-b3db-4bfe-ef94-45ebd71bd5e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0827 09:10:59.607845 140075589404480 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:466: BaseResourceVariable.constraint (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Apply a constraint manually following the optimizer update step.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 19s 325us/sample - loss: 56.3932 - val_loss: 50.7875\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 15s 252us/sample - loss: 49.4623 - val_loss: 48.3936\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 15s 252us/sample - loss: 47.5146 - val_loss: 46.1677\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 15s 251us/sample - loss: 46.3738 - val_loss: 46.5257\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 15s 250us/sample - loss: 45.6400 - val_loss: 45.4343\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 15s 251us/sample - loss: 45.0722 - val_loss: 45.5885\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 15s 250us/sample - loss: 44.6297 - val_loss: 44.2402\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 15s 250us/sample - loss: 44.2514 - val_loss: 44.4669\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 15s 251us/sample - loss: 43.9406 - val_loss: 43.9488\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 15s 252us/sample - loss: 43.7155 - val_loss: 43.8077\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 15s 252us/sample - loss: 43.4535 - val_loss: 43.8017\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 15s 251us/sample - loss: 43.2580 - val_loss: 43.6468\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 15s 252us/sample - loss: 43.0848 - val_loss: 43.3025\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 15s 251us/sample - loss: 42.9231 - val_loss: 43.5017\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 15s 252us/sample - loss: 42.7545 - val_loss: 43.2054\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 15s 251us/sample - loss: 42.6434 - val_loss: 43.5704\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 15s 252us/sample - loss: 42.4934 - val_loss: 42.6333\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 15s 253us/sample - loss: 42.3669 - val_loss: 42.3606\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 15s 254us/sample - loss: 42.2738 - val_loss: 42.7189\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 15s 251us/sample - loss: 42.1704 - val_loss: 42.5619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f65400c51d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tensorflow.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "x_train = numpy.reshape(x_train, newshape=(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1))\n",
    "x_test = numpy.reshape(x_test, newshape=(x_test.shape[0], x_train.shape[1], x_train.shape[2], 1))\n",
    "\n",
    "vae.fit(x_train, x_train, epochs=20, batch_size=32, shuffle=True, validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0L-lAAmolar9"
   },
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v-YedmB3kxM8"
   },
   "outputs": [],
   "source": [
    "encoder.save(\"VAE_encoder.h5\")\n",
    "decoder.save(\"VAE_decoder.h5\")\n",
    "vae.save(\"VAE.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b_R3UcEGlY8B"
   },
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "80jO7_Jcky1g"
   },
   "outputs": [],
   "source": [
    "encoder = tensorflow.keras.models.load_model(\"VAE_encoder.h5\", compile=False)\n",
    "decoder = tensorflow.keras.models.load_model(\"VAE_decoder.h5\", compile=False)\n",
    "\n",
    "# Preparing MNIST Dataset\n",
    "(x_train, y_train), (x_test, y_test) = tensorflow.keras.datasets.mnist.load_data()\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "x_test = numpy.reshape(x_test, newshape=(x_test.shape[0], x_train.shape[1], x_train.shape[2], 1))\n",
    "\n",
    "encoded_data = encoder.predict(x_test)\n",
    "decoded_data = decoder.predict(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
